"""
MoE Architecture Test Suite
Generated by MoE-Architecture-Validator

Tests for validating MoE routing, capacity constraints, and load balancing.
"""

import torch
import pytest
import numpy as np
import math
from typing import Dict, List, Tuple

# Assuming MoE implementation is importable
try:
    from src.cdrmix.router import MoERouter
    from src.cdrmix.moe_block import (
        MoERWKVBlock, MoETransformerBlock, 
        ExpertParameterCalculator, RWKVExpert, TransformerExpert
    )
except ImportError:
    pytest.skip("MoE implementation not available", allow_module_level=True)


class TestMoERouting:
    """Test MoE routing algorithm correctness."""
    
    def test_router_outputs_exact_topk(self):
        """Test that router outputs exactly top-k experts per token."""
        batch_size, seq_len, hidden_dim = 4, 16, 512
        num_experts, top_k = 8, 2
        
        router = MoERouter(d_model=hidden_dim, num_experts=num_experts, top_k=top_k)
        x = torch.randn(batch_size, seq_len, hidden_dim)
        
        routing_weights, selected_experts, aux_losses = router(x)
        
        # Verify exactly top_k experts selected per token
        assert selected_experts.shape[-1] == top_k
        
        # Verify routing weights sum to 1 for each token (they are already normalized)
        weight_sums = routing_weights.sum(dim=-1)
        assert torch.allclose(weight_sums, torch.ones_like(weight_sums), rtol=1e-5)
    
    def test_capacity_constraints_respected(self):
        """Test that capacity constraints from Lean spec are respected."""
        batch_size, seq_len = 8, 32
        num_tokens = batch_size * seq_len
        num_experts, top_k = 8, 2
        capacity_factor = 1.25
        
        # Calculate expected capacity per expert (from Lean spec)
        expected_capacity = math.ceil(capacity_factor * (num_tokens * top_k) / num_experts)
        
        router = MoERouter(d_model=512, num_experts=num_experts, top_k=top_k, capacity_factor=capacity_factor)
        x = torch.randn(batch_size, seq_len, 512)
        
        routing_weights, selected_experts, aux_losses = router(x)
        
        # Count tokens assigned to each expert
        expert_counts = torch.zeros(num_experts)
        flat_experts = selected_experts.flatten()
        
        for expert_id in range(num_experts):
            expert_counts[expert_id] = (flat_experts == expert_id).sum().item()
        
        # Verify no expert exceeds capacity
        max_expert_load = expert_counts.max().item()
        assert max_expert_load <= expected_capacity, f"Expert overload: {max_expert_load} > {expected_capacity}"
    
    def test_load_balancing_effectiveness(self):
        """Test that load balancing distributes tokens reasonably."""
        batch_size, seq_len = 16, 64
        num_experts, top_k = 8, 2
        capacity_factor = 1.25
        
        router = MoERouter(d_model=512, num_experts=num_experts, top_k=top_k, capacity_factor=capacity_factor)
        x = torch.randn(batch_size, seq_len, 512)
        
        routing_weights, selected_experts, aux_losses = router(x)
        
        # Calculate load distribution
        expert_loads = torch.zeros(num_experts)
        flat_experts = selected_experts.flatten()
        
        for expert_id in range(num_experts):
            expert_loads[expert_id] = (flat_experts == expert_id).sum().item()
        
        # Check load balance (coefficient of variation should be reasonable)
        mean_load = expert_loads.mean()
        std_load = expert_loads.std()
        cv = std_load / mean_load if mean_load > 0 else float('inf')
        
        # CV should be < 0.5 for reasonable load balancing
        assert cv < 0.5, f"Poor load balancing: CV={cv:.3f}"
    
    def test_routing_determinism(self):
        """Test that routing is deterministic for same inputs."""
        torch.manual_seed(42)
        
        router = MoERouter(d_model=512, num_experts=8, top_k=2)
        router.eval()  # Set to eval mode to disable dropout
        x = torch.randn(4, 8, 512)
        
        # Two forward passes with same input
        weights1, experts1, aux1 = router(x)
        weights2, experts2, aux2 = router(x)
        
        assert torch.allclose(weights1, weights2)
        assert torch.equal(experts1, experts2)
    
    def test_gradient_flow_through_routing(self):
        """Test that gradients flow properly through sparse routing."""
        router = MoERouter(d_model=512, num_experts=8, top_k=2)
        x = torch.randn(2, 4, 512, requires_grad=True)
        
        routing_weights, selected_experts, aux_losses = router(x)
        
        # Create dummy loss
        loss = routing_weights.sum()
        loss.backward()
        
        # Check that input gradients exist and are reasonable
        assert x.grad is not None
        assert not torch.isnan(x.grad).any()
        assert not torch.isinf(x.grad).any()


class TestMoEBlock:
    """Test complete MoE block functionality."""
    
    def test_moe_block_forward_pass(self):
        """Test that MoE block processes input correctly."""
        batch_size, seq_len, hidden_dim = 4, 16, 512
        
        moe_block = MoERWKVBlock(
            d_model=hidden_dim,
            num_experts=8,
            top_k=2
        )
        
        x = torch.randn(batch_size, seq_len, hidden_dim)
        output, state = moe_block(x, return_aux_loss=False)
        
        # Output should have same shape as input
        assert output.shape == x.shape
        
        # Output should not be NaN or Inf
        assert not torch.isnan(output).any()
        assert not torch.isinf(output).any()
    
    def test_expert_specialization(self):
        """Test that different experts produce different outputs."""
        hidden_dim = 512
        moe_block = MoERWKVBlock(
            d_model=hidden_dim,
            num_experts=8,
            top_k=1  # Use only one expert at a time
        )
        
        x = torch.randn(1, 1, hidden_dim)
        
        # Force different experts and compare outputs
        outputs = []
        for expert_id in range(8):
            # This would need expert forcing mechanism in implementation
            # outputs.append(moe_block.forward_with_expert(x, expert_id))
            pass
        
        # Check that expert outputs differ significantly
        # This test requires implementation of expert forcing
        pass


class TestMoEMathematicalProperties:
    """Test mathematical properties from Lean specification."""
    
    def test_capacity_sufficient_theorem(self):
        """Test the capacity_sufficient theorem from Lean spec."""
        # This test validates the mathematical theorem
        N = 1000  # number of tokens
        E = 8  # number of experts  
        top_k = 2
        phi = 1.25  # capacity factor >= 1
        
        # Per-expert capacity from Lean spec
        per_expert_capacity = math.ceil(phi * (N * top_k) / E)
        
        # Expected load under perfect balance
        expected_load_per_expert = math.ceil((N * top_k) / E)
        
        # Theorem: if phi >= 1, then capacity >= expected load
        assert per_expert_capacity >= expected_load_per_expert
    
    def test_no_overflow_property(self):
        """Test NoOverflow property from Lean specification."""
        # This would be tested at runtime with actual routing
        # For now, verify the mathematical constraint
        
        N = 500
        E = 8
        top_k = 2  
        phi = 1.25
        
        capacity = math.ceil(phi * (N * top_k) / E)
        total_capacity = capacity * E
        total_assignments = N * top_k
        
        # Total capacity should accommodate all assignments
        assert total_capacity >= total_assignments


class TestExpertParameterCalculations:
    """Test expert parameter calculations for different model scales."""
    
    def test_should_calculate_1b_model_expert_dimensions(self):
        """Test expert parameter calculations for 1B model scale."""
        config = ExpertParameterCalculator.get_model_scale_config('1b')
        
        # Verify base configuration
        assert config['total_params'] == 1_000_000_000
        assert config['d_model'] == 2048
        assert config['n_layers'] == 24
        assert config['n_heads'] == 16
        
        # Target: 125M parameters per expert (1B / 8 experts)
        target_per_expert = 125_000_000
        actual_per_expert = config['actual_params_per_expert']
        
        # Should be within 20% tolerance for practical sizing
        tolerance = 0.20
        assert abs(actual_per_expert - target_per_expert) <= tolerance * target_per_expert
        
        # Verify expert_d_ff is reasonable
        assert config['expert_d_ff'] > config['d_model']  # Should expand hidden size
    
    def test_should_calculate_4b_model_expert_dimensions(self):
        """Test expert parameter calculations for 4B model scale."""
        config = ExpertParameterCalculator.get_model_scale_config('4b')
        
        assert config['total_params'] == 4_000_000_000
        assert config['d_model'] == 3072
        
        # Target: 500M parameters per expert (4B / 8 experts)
        target_per_expert = 500_000_000
        actual_per_expert = config['actual_params_per_expert']
        tolerance = 0.20
        
        assert abs(actual_per_expert - target_per_expert) <= tolerance * target_per_expert
    
    def test_should_calculate_40b_model_expert_dimensions(self):
        """Test expert parameter calculations for 40B model scale."""
        config = ExpertParameterCalculator.get_model_scale_config('40b')
        
        assert config['total_params'] == 40_000_000_000
        assert config['d_model'] == 6144
        
        # Target: 5B parameters per expert (40B / 8 experts)
        target_per_expert = 5_000_000_000
        actual_per_expert = config['actual_params_per_expert']
        tolerance = 0.20
        
        assert abs(actual_per_expert - target_per_expert) <= tolerance * target_per_expert
    
    def test_should_calculate_200b_model_expert_dimensions(self):
        """Test expert parameter calculations for 200B model scale."""
        config = ExpertParameterCalculator.get_model_scale_config('200b')
        
        assert config['total_params'] == 200_000_000_000
        assert config['d_model'] == 12288
        
        # Target: 25B parameters per expert (200B / 8 experts)
        target_per_expert = 25_000_000_000
        actual_per_expert = config['actual_params_per_expert']
        tolerance = 0.20
        
        assert abs(actual_per_expert - target_per_expert) <= tolerance * target_per_expert
    
    def test_should_handle_invalid_model_scales(self):
        """Test error handling for invalid model scales."""
        with pytest.raises(ValueError, match="Unknown model scale"):
            ExpertParameterCalculator.get_model_scale_config('invalid_scale')
    
    def test_parameter_scaling_consistency(self):
        """Test that parameter scaling is consistent across model sizes."""
        scales = ['1b', '4b', '40b', '200b']
        configs = [ExpertParameterCalculator.get_model_scale_config(scale) for scale in scales]
        
        # Verify that larger models have proportionally larger expert sizes
        for i in range(1, len(configs)):
            curr_config = configs[i]
            prev_config = configs[i-1]
            
            # Current model should have more total parameters
            assert curr_config['total_params'] > prev_config['total_params']
            
            # Expert parameters should scale proportionally
            assert curr_config['actual_params_per_expert'] > prev_config['actual_params_per_expert']
            
            # Model dimension should increase
            assert curr_config['d_model'] >= prev_config['d_model']


class TestRWKVExpertImplementation:
    """Test RWKV expert component implementation."""
    
    def test_rwkv_expert_should_have_correct_structure(self):
        """Test RWKV expert uses ChannelMix component."""
        d_model, expert_d_ff = 512, 1024
        expert = RWKVExpert(d_model, expert_d_ff)
        
        # Should have ChannelMix component
        assert hasattr(expert, 'channel_mix')
        assert expert.d_model == d_model
        assert expert.expert_d_ff == expert_d_ff
    
    def test_rwkv_expert_forward_pass_correctness(self):
        """Test RWKV expert forward computation produces correct outputs."""
        d_model, expert_d_ff = 256, 512
        expert = RWKVExpert(d_model, expert_d_ff)
        
        batch_size, seq_len = 2, 8
        x = torch.randn(batch_size, seq_len, d_model)
        
        output = expert(x)
        
        # Output shape should match input
        assert output.shape == (batch_size, seq_len, d_model)
        
        # Output should not be NaN or Inf
        assert not torch.isnan(output).any()
        assert not torch.isinf(output).any()
        
        # Output should be different from input (transformation occurred)
        assert not torch.allclose(output, x, atol=1e-6)
    
    def test_rwkv_expert_parameter_count_accuracy(self):
        """Test parameter counting matches expected ChannelMix parameters."""
        d_model, expert_d_ff = 512, 1024
        expert = RWKVExpert(d_model, expert_d_ff)
        
        param_count = expert.get_parameter_count()
        
        # Expected parameters in ChannelMix:
        # - key_proj: d_model * expert_d_ff
        # - value_proj: expert_d_ff * d_model  
        # - receptance_proj: d_model * d_model
        expected_params = (
            d_model * expert_d_ff +  # key_proj
            expert_d_ff * d_model +  # value_proj
            d_model * d_model        # receptance_proj
        )
        
        assert param_count == expected_params
    
    def test_rwkv_expert_with_different_sizes(self):
        """Test RWKV expert works with different model scale dimensions."""
        test_configs = [
            (512, 1024),    # Small model
            (2048, 4096),   # 1B scale
            (3072, 6144),   # 4B scale
            (6144, 12288)   # 40B scale
        ]
        
        for d_model, expert_d_ff in test_configs:
            expert = RWKVExpert(d_model, expert_d_ff)
            
            # Test forward pass
            x = torch.randn(1, 4, d_model)
            output = expert(x)
            
            assert output.shape == x.shape
            assert not torch.isnan(output).any()
            
            # Verify parameter count scales correctly
            param_count = expert.get_parameter_count()
            expected = d_model * expert_d_ff * 2 + d_model * d_model
            assert param_count == expected


class TestTransformerExpertImplementation:
    """Test Transformer expert component implementation."""
    
    def test_transformer_expert_should_have_correct_structure(self):
        """Test Transformer expert uses FFN component."""
        d_model, expert_d_ff = 512, 2048
        expert = TransformerExpert(d_model, expert_d_ff)
        
        # Should have FFN component
        assert hasattr(expert, 'ffn')
        assert expert.d_model == d_model
        assert expert.expert_d_ff == expert_d_ff
    
    def test_transformer_expert_forward_pass_correctness(self):
        """Test Transformer expert forward computation."""
        d_model, expert_d_ff = 256, 1024
        expert = TransformerExpert(d_model, expert_d_ff)
        
        batch_size, seq_len = 2, 8
        x = torch.randn(batch_size, seq_len, d_model)
        
        output = expert(x)
        
        # Output shape should match input
        assert output.shape == (batch_size, seq_len, d_model)
        
        # Output should not be NaN or Inf
        assert not torch.isnan(output).any()
        assert not torch.isinf(output).any()
        
        # Output should be different from input (transformation occurred)
        assert not torch.allclose(output, x, atol=1e-6)
    
    def test_transformer_expert_parameter_count_accuracy(self):
        """Test parameter counting for Transformer expert FFN."""
        d_model, expert_d_ff = 512, 2048
        expert = TransformerExpert(d_model, expert_d_ff)
        
        param_count = expert.get_parameter_count()
        
        # Expected parameters in TransformerFFN:
        # - Linear layers with bias terms
        # - Gate, up, and down projections
        # This should match the TransformerFFN implementation
        assert param_count > 0
        assert isinstance(param_count, int)


class TestMoEBlockIntegration:
    """Test complete MoE block integration with different expert types."""
    
    def test_moe_rwkv_block_with_scaled_experts(self):
        """Test MoE-RWKV block with properly scaled experts."""
        # Use 1B model configuration
        config = ExpertParameterCalculator.get_model_scale_config('1b')
        
        moe_block = MoERWKVBlock(
            d_model=config['d_model'],
            num_experts=8,
            top_k=2,
            expert_d_ff=config['expert_d_ff']
        )
        
        batch_size, seq_len = 2, 16
        x = torch.randn(batch_size, seq_len, config['d_model'])
        
        # Test forward pass
        output, state = moe_block(x, return_aux_loss=False)
        
        assert output.shape == x.shape
        assert state is not None  # RWKV state
        assert not torch.isnan(output).any()
        
        # Test with auxiliary losses
        output, state, aux_losses = moe_block(x, return_aux_loss=True)
        
        assert aux_losses is not None
        assert 'load_balance' in aux_losses
        assert 'z_loss' in aux_losses
        assert 'overflow' in aux_losses
    
    def test_moe_transformer_block_with_scaled_experts(self):
        """Test MoE-Transformer block with properly scaled experts."""
        config = ExpertParameterCalculator.get_model_scale_config('4b')
        
        moe_block = MoETransformerBlock(
            d_model=config['d_model'],
            n_heads=config['n_heads'],
            num_experts=8,
            top_k=2,
            expert_d_ff=config['expert_d_ff']
        )
        
        batch_size, seq_len = 2, 16
        x = torch.randn(batch_size, seq_len, config['d_model'])
        
        # Test forward pass
        output = moe_block(x, return_aux_loss=False)
        
        assert output.shape == x.shape
        assert not torch.isnan(output).any()
        
        # Test with auxiliary losses
        output, aux_losses = moe_block(x, return_aux_loss=True)
        
        assert aux_losses is not None
        assert isinstance(aux_losses, dict)
    
    def test_expert_statistics_computation(self):
        """Test expert usage statistics for monitoring."""
        moe_block = MoERWKVBlock(
            d_model=512,
            num_experts=8,
            top_k=2,
            expert_d_ff=1024
        )
        
        x = torch.randn(4, 16, 512)
        
        stats = moe_block.get_expert_statistics(x)
        
        # Verify all required statistics are present
        required_keys = [
            'expert_parameter_counts', 'total_expert_parameters',
            'average_parameters_per_expert', 'num_experts', 'top_k',
            'entropy', 'max_expert_load', 'expected_load'
        ]
        
        for key in required_keys:
            assert key in stats, f"Missing statistic: {key}"
        
        # Verify statistics are reasonable
        assert len(stats['expert_parameter_counts']) == 8
        assert all(count > 0 for count in stats['expert_parameter_counts'])
        assert stats['total_expert_parameters'] > 0
        assert stats['num_experts'] == 8
        assert stats['top_k'] == 2


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
